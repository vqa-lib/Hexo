---
title: ICME2016 学术动态与参会经验分享
date: 2016-10-07 16:41:40
tags: [学术会议]
categories: 学术动态
---
会议简介
====

ICME，全称International Conference on Multimedia and Expo，即国际多媒体与博览会议，又IEEE 计算机学会、电路与系统学会、通信学会、信号处理学会合办，始于2000年，不仅是学术界交流多媒体领域最近研究成果的旗舰论坛，也是工业界展示最新产品或系统的平台。2016年ICME会议于7月11日至7月15日在美国西雅图召开。
在本次会议中，我们重点听取了以下内容：

1、A Quest for Visual Intelligence in Computers- Fei-Fei Li, Associate Professor, Stanford University
------------------------------------------------------------------------

Fei-Fei Li是计算机视觉领域的大牛，由她主导的imagenet图像库是机器视觉领域最大最全的图像库，基于此举办的挑战赛更是吸引全世界的学者参与进来。在本次讲座中，她从视觉引发的物种大爆发讲起，梳理了人们在追寻让计算机更智能地理解图片和视频的道路上所做的努力，也介绍了她所在的实验室在这一领域的最新成果，她也提到很多成果都是以大数据和神经网络（深度学习）作为基础的。

Fei-Fei Li首先指出，计算机视觉的目标在于Total Scene Understanding，在这一过程中势必涉及到对图片或视频像素的处理，但是简单的measuring pixels不等于understanding scenes，这里面涉及到动物的视觉感知的原理，很多生物实验都给出了证明，比如著名的the kitten experiment，实验中的小猫只会对以前见过的图像做出生理上的反应，这些实验给我们的第一个有关计算机视觉的信息是
Message1 learning is the path to visual intelligence
这也是如今计算机视觉和机器学习紧密联系在一起的根本原因之一。

在大的目标下，计算机视觉领域有很多子目标，object recognition就是其中之一，随后Fei-Fei Li就目标识别领域数十年来的发展轨迹做了介绍，包括最新的进展。

她指出第二个有关计算机视觉的关键信息是
Message2 Learning requires Big Data
将大数据应用于计算机视觉的典型例子就是imageNet，包含了22000个类别的15000000张图片，而基于该图片库的The image classification challenge中，也是随着卷积神经网络的运用而取得了突破性的进展，这无疑再次强调了机器学习的强大。讲到这里，Fei-Fei Li又引出了object detection的话题，并提出了其中的难点：small and texture less objects are very difficult to detect。

计算机视觉的第二个子目标是Gist of a scene aka. Image captioning，也就是图像、视频的理解，由计算机自动判断出一个场景包含的内容的主旨，比如输入一张照片，计算就能给出“这是一个穿红衣服的小女孩在玩气球”的结果。这其中就有两个关键问题，how to process image以及how to generate sentence，而这两个问题的答案又是神经网络，事实上，a deep understanding requires knowledge and texture.

演讲的Slides可以在[这里](http://www.ieee-icme.org/ICME2016/www.icme2016.org/files/plenary/FeiFeiLi.pdf)下载。

2、Quality of Experience in Multimedia Systems and Services
----------------------------------------------------------

本讲座的内容包含三个部分，首先是QoE的总体介绍，然后是DASH中的QoE，最后是一个新的概念-Quality of Life。这部分的内容因为有一定的基础，所以主要以补充为主。
比如在第一部分中，我们了解到有一个Qualinet联盟（www.qualinet.eu），致力于推动工业界和学术界对QoE的重视。此外，我们还了解到ISO 9000系列都是与质量管理有关的标准集，MPEG组织也有自己的一套主观质量评价标准，ITU-FSG12是与QoE有关的标准，我们还了解到QoE评价的关键在于collect users feedback，这也体现在第二部分中的crowd source QoE evaluation中。
QoE的modeling总结如下图
![][1]


事实上，QoE也不是纸上谈兵，已经有公司切实地将其付诸于行动了，如下例
![][2]


  比较有意思的是，因为切换频率同样会严重影响QoE，所以我们看到的分片时长最短就是2s，这也是因为再短的分片时长会导致频繁的切换，给人闪烁的感觉。
  

3、Making the Virtual Real – The future of Augmented and Virtual Reality
-----------------------------------------------------------------------

这一讲座是由谷歌、微软、Valve的技术人员带来的有关VR、AR的内容。
VR、AR的核心在于为用户提供沉浸式的体验，这种体验还包括与周边环境的交流，是一种增强的感知体验。除去一些科普性的内容之外，谈到了一些业界面临的挑战，比如头戴式设备的sensing、如何与现实世界co-exist、安全问题、静态和动态场景重建问题、人体模型重建问题、低功耗低成本问题。

4、Best Student Paper Award Session
----------------------------------

这部分是最佳学生论文奖的竞赛部分，共有四位提名者，都是来自中国的学生，非常自豪，但是不得不说中国学生的英文演讲水平真的有待提高啊~
第一篇提名论文是关于车辆识别的，本质还是检索，总结起来就是既有简单的特征（纹理、颜色等）检索，又有基于深度学习的更为复杂的high level检索，很值得学习。

![][3]

第二篇的内容比较偏，略过。
第三篇提名论文是关于质量评价的，很有意思，不同于以往的算法都是将受损图像和最优质量图像对比得出评价结果，这篇论文是和原始图片的最差质量比较，
![][4]
![][5]
![][6]
从压缩到最差级别的图像中提取出了一个所谓的pseudo structural similarity的概念，利用这一指标进行评价
![][7]
![][8]
而且他们的评价算法进行一些优化之后实现以19fps的速度对4k分辨率的图像进行质量评价。

第四篇提名论文是对HEVC编码器的优化，用贝叶斯分类算法做帧内编码的决策，贝叶斯分类也是很多分类算法的基础，在数据挖掘等领域也用的很多，也很有启发性。

5、bitmovin Grand Challenge on Dynamic Adaptive Streaming over HTTP
------------------------------------------------------------------

这一部分是由bitmovin公司主办的挑战赛，需要参赛者设计DASH的自适应算法，并且在指定的数据集（QoMEX、Qualinet Dataset）上进行测试。有三篇文章提名，其中两篇来自中国，都是北大的学生，第一篇利用了马尔科夫模型，第二篇基于控制论，在客户端设计了动态变化的buffer，还利用了fast-start batch downloading的技术，加快MPD和初始seg的下载。两篇都规规矩矩，但是做汇报的学生态度感觉一般，不严肃不认真。
第三篇是将DASH的自适应机制和人口出生率死亡率做了一个类比然后提出了一个算法，非常新颖，
![][9]
但是因为主讲者是一位黑人朋友，口音比较重，本来概念就比较新颖，再加上理解成本，基本没怎么听懂，准备详细阅读一下他的论文后再做整理。
需要注意的是，这个grand challenge明年还会举办~
![][10]

6、Ultra HD – Roadmap of High Qualiry A/V Content to the Home
------------------------------------------------------------

这是一个工业论坛，由来自Dolby、Sony、LG的技术人员给出关于超高清内容、HDR内容的制作、传输等方面的讲座。其实这和传媒大学所谓的特色-广播电视工程专业非常相符

4k的规格在这里简单提一下，

 >* 4K-UHDTV-1 standard 8 Megapixels 4k 
 >* 4k-UHDTV-2 standard 33Mefapixels 8k

这里HDR的内容比较值得深入研究，比如更宽的亮度和色度范围可以带来true black和true white，EOTF、perceptual quantizer、8bit\14bit gamma、SMPTE2084等等内容，都很陌生，但也很有兴趣，其实不只是客厅，移动端上的安卓最新版也提到了对HDR的支持。
其实总结起来下一代的客厅娱乐需要考虑的是三大因素：nits、bits、bucks（成本），这一总结很有意思。
![][11]
![][12]
讲座的第二部分是HDR production overview，HDR的内容制作是一个缓慢发展的领域，其中的一个点就是color depth的提升，从8bit到10bit再到12bit。同时包含了HDR内容传输的解决方案。
讲座的第三部分是显示技术，即rendering better pixels，其实也有点老生常谈，无非就是更优的亮度、对比度、观看角度。当然也有一些专为人眼视觉系统优化的技术，比如chroma subsampling、new pixel structure（还可以提升power utilization efficiency），sub-pixel rendering等等。
讲座的最后部分是HDR dynamic metadata，由dolby提出，可以track内容的变化，可以做tone\color volume mapping
HDR同样面临一些挑战，他大约会导致25%的码率增幅，所以码率可能会成为一个挑战，而dolby vision技术使用metadata可以在HD或4K内容上带来HDR体验，考虑到码率问题，也许HD+HDR就足够了。还有一个partial HDR的概念也值得关注。

7、Packet Video Workshop2016 – Coding for Augmented and Virtual Reality
----------------------------------------------------------------------

由微软的技术人员带来的VR和AR内容编码方法讲座。讲座最开始先科普了一下VR和AR在capture技术上的区别，前者是inside out，后者则是outside in（关于AR capture技术的介绍，可以参考siggraph中MS的演示视频，在youtube上应该也能搜到，很有意思；还有，AR是不会产生motion sickness的），同时还介绍了一下最新的holoportation技术，非常astonishing，可以将不在同一地点的人的实时虚拟模型进行传送，并且可以互动。
至于具体的编码方法，说实话，因为涉及到很多数学东西，没有怎么听懂，但还是能发现和传统视频编码的套路类似，去高频、留低频，即可实现coding、prediction、interpolation，只不过对应VR、AR特殊的应用场景会有各种映射、point cloud、mesh等等内容。有兴趣详细了解的朋友，可以去搜索相应的论文。
不过VR|AR内容的编码还有一些问题，比如缺乏dataset、reference codec以及distortion codec，不过就像过去的coding method，面包会有的，一切都会有的。此外，针对VR、AR内容的质量评价，需要注意的是PSNR不再有效，这也是as a result of mesh，一点点的moving动作就会导致画面大大不同，PSNR也会剧变，这里面其实也能看到VR、AR内容与传统视频内容的一大区别。

此外再提一点，在另外一个讲座”Mulsemedia”-based Collaborative Mixed/Virtual Reality Environments中，提到了VR中的一些更细节的问题，比如camrea标定（使用棋盘格，对齐左右眼的RGB data和deep information），动作recognition（不同关节、左右识别，因为user may not always facing camera）、deep scale（也即是在camrea中实现近大远小的基本物理原理）、noise removal、depth image based meshing point cloud等等。

8、DASH special session
----------------------

这部分就是各个有关dash的论文的presentation了，总结起来，有这么几个研究方向，一个是自适应算法、一个是针对dash特点做的编解码器优化、一个是信道传输上的带宽估计和利用率优化，最后一个就是服务器端的优化。
8.1 Efficient Lightweight Video Packet Filtering for Large-Scale Video Data Delivery
这篇文章的主题是讨论如何coping with bandwidth shortage?给出的解决方案就是blocking some frames to match avail. Bandwidth,那么这其中就涉及一个filtering strategy了，同样是一个最优化问题，given loss ratio, block less important frames to max. QoE.具体来说是在container中加上metadata，这一metadata contain important score for every frame,这个score是根据frame type\size\dependency\distortion来计算的。其中的distortion是用一种chain mode来计算的，最后的验证是用MS-SSIM来作为指标的。
8.2 Low Delay MPEG DASH Streaming over the WebRTC Data Channel
其实就是用webrtc的channel来carry dash video session, change pull mode to push mode，原本的dash传输中的delay问题主要包含两个部分，一个是segment dur，一个是TCP delay，这里很大程度上是因为TCP的slow start策略，而通过使用webrtc channel，就能消除这部分TCP delay，从而达到low delay的目标。
8.3 DASH Sub-Representation with Temporal QoE Driven Layering
这一篇很有新意，普遍来说大家都认为DASH的sub-rep没什么用，这篇论文就给我们展示了它的用处，详细的内容我还没理解透，大致来说也是给不同的帧加上important factor，给更重要的帧更高的优先级。
8.4 Adaptive Media Playout Assited Rate Adaptation Scheme for HTTP Adaptive Streaming over LTE System
简单来说是两个步骤：一是guarantee playback continuity，二是utilize residual resource
8.5 Boosting Decoding Quality Performance in DASH-based Streaming Frameworks 
再一次的，给不同的帧不同的重要级别，不过在这个判断重要性的过程中应用到了machine learning的内容。整篇论文的内容更多的是关于codec characteristic的，以PSNR和SSIM作为评价指标，基于HEVC做的实现，只增加解码端的复杂度来获得总体质量上的提升。
8.6 History-based Throughput Prediction with Hidden Markov Model in Mobile Networks
使用隐马尔科夫模型做带宽预测。传统的预测模型基于RTT、lossrate，不考虑历史流量，而history-based模型基于stochastic prediction model和GMM-HMM。在具体的计算过程中还涉及到forward-backward algo和viterbi model。最后的验证试验不仅在real network中进行，还在各种各样的network traces中进行。这一点很值得学习。
8.7 A Dynamic and Complexity Aware Cloud Scheduling Algorithm for Video Transcoding
是基于Apach Hadop的云转码的效率提升算法，主要是任务调度算法，传统的任务调度算法有FIFO、FAIR Schedule和capacity schedule。这里做的improvement就是根据复杂度决定优先级，复杂度有转码视频的帧数决定。根据cpu utilization的情况动态改变云转码系统中的slot number。还有一个转码时间的优化方案：将大文件以48-64MB为单位切割为segment，分大块和小块调度下载优先级，类似于迅雷的空闲下载。作者还指出，这样的调度算法也可以用于DASH的live streaming框架。


  [1]: http://img.blog.csdn.net/20160722224555564?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center
  [2]: http://img.blog.csdn.net/20160722224621892?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center
  [3]: http://img.blog.csdn.net/20160722224918578?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center
  [4]: http://img.blog.csdn.net/20160722225435316?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center
  [5]: http://img.blog.csdn.net/20160722225709114?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center
  [6]: http://img.blog.csdn.net/20160722225735864?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center
  [7]: http://img.blog.csdn.net/20160722225924412?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center
  [8]: http://img.blog.csdn.net/20160722225937052?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center
  [9]: http://img.blog.csdn.net/20160722230110055?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center
  [10]: http://img.blog.csdn.net/20160722230147819?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center
  [11]: http://img.blog.csdn.net/20160722230758970?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center
  [12]: http://img.blog.csdn.net/20160722230843674?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center